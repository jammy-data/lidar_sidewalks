{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae3d664",
   "metadata": {},
   "source": [
    "# Multi-city classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be03e6",
   "metadata": {},
   "source": [
    "We need to run a classifier that has been tested on one pointcloud, and test it on another to measure the accuracy.\n",
    "\n",
    "What is difficult to determine, is how effective this model will be with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "import laspy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt     \n",
    "import seaborn as sns\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from upath import UPath\n",
    "import os\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup remote B2Drop file path ---\n",
    "B2D_DIR = UPath(\n",
    "    os.getenv(\"DATA_DIR_FSSPEC_URI\"),\n",
    "    base_url=os.getenv(\"DATA_DIR_FSSPEC_BASE_URL\"),\n",
    "    auth=(os.getenv(\"DATA_DIR_FSSPEC_USER\"),\n",
    "          os.getenv(\"DATA_DIR_FSSPEC_PASS\"))\n",
    ")\n",
    "file_path = B2D_DIR / \"bologna.laz\"\n",
    "\n",
    "# --- Read remote file as binary stream ---\n",
    "with file_path.open(\"rb\") as f:\n",
    "    las_bytes = f.read()  # careful: this still streams the full file once\n",
    "from pathlib import Path\n",
    "\n",
    "# Write bytes to a temp file\n",
    "temp_file = Path(\"../data/bologna.laz\")\n",
    "with open(temp_file, \"wb\") as f:\n",
    "    f.write(las_bytes)\n",
    "\n",
    "# Run PDAL on it\n",
    "import pdal, json\n",
    "\n",
    "pipeline_json = {\n",
    "    \"pipeline\": [\n",
    "        {\"type\": \"readers.las\", \"filename\": str(temp_file)},\n",
    "        {\"type\": \"filters.stats\"},\n",
    "        {\"type\": \"writers.las\", \"filename\": \"../data/bologna_filtered.las\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "pipeline.execute()\n",
    "print(\"PDAL processed file from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa83d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDAL processed file from disk\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# --- Settings ---\n",
    "input_file = las_bytes              # input LiDAR\n",
    "output_dir = Path(\"../data/bologna_tiles/parquet\") # output folder\n",
    "points_per_chunk = 1_000_000                    # max points per tile\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Read LAS/LAZ ---\n",
    "las = laspy.read(input_file)\n",
    "num_points = len(las.x)\n",
    "print(f\"Loaded {num_points} points\")\n",
    "\n",
    "# --- Calculate number of chunks ---\n",
    "num_chunks = math.ceil(num_points / points_per_chunk)\n",
    "print(f\"Splitting into {num_chunks} chunks of up to {points_per_chunk} points each\")\n",
    "\n",
    "# --- Process each chunk ---\n",
    "for i in range(num_chunks):\n",
    "    start = i * points_per_chunk\n",
    "    end = min((i + 1) * points_per_chunk, num_points)\n",
    "\n",
    "    # Slice the points\n",
    "    chunk = las[start:end]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"x\": chunk.x,\n",
    "        \"y\": chunk.y,\n",
    "        \"z\": chunk.z,\n",
    "        \"intensity\": chunk.intensity,\n",
    "        \"return_number\": chunk.return_number,\n",
    "        \"classification\": chunk.classification\n",
    "    })\n",
    "\n",
    "    # Write to Parquet\n",
    "    output_file = output_dir / f\"chunk_{i:03d}.parquet\"\n",
    "    df.to_parquet(output_file, engine=\"pyarrow\", index=False)\n",
    "\n",
    "    print(f\"Chunk {i+1}/{num_chunks}: wrote {len(df)} points to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821300aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar-sidewalks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
